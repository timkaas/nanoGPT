{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:17.009237Z",
     "start_time": "2023-07-25T22:20:16.985662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import pathlib as pl\n",
    "from random import randrange, shuffle, randint\n",
    "import random\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer, Encoding\n",
    "from transformers import RobertaTokenizerFast, BertTokenizerFast, BatchEncoding\n",
    "\n",
    "import seqeval\n",
    "from seqeval import metrics\n",
    "\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm  # for our loading bar\n",
    "import plotly.express as px\n",
    "\n",
    "# Local modules\n",
    "from masking_dataset import MaskingDataset\n",
    "from ner_dataset import NerDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.542454Z",
     "start_time": "2023-07-25T22:20:17.009904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime(\"%Y%m%d\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.604430Z",
     "start_time": "2023-07-25T22:20:31.543555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_dir = pl.Path.cwd() / \"data\"\n",
    "model_dir = pl.Path.cwd() / \"model\"\n",
    "plot_dir = pl.Path.cwd() / \"plot\"\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "oscar_dir = data_dir / \"oscar\"\n",
    "oscar_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer_dir = model_dir / \"tokenizer\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.620887Z",
     "start_time": "2023-07-25T22:20:31.577111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return itertools.chain.from_iterable(l)\n",
    "\n",
    "def get_line_count(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        line_count = sum(1 for _ in file)\n",
    "    return line_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.642894Z",
     "start_time": "2023-07-25T22:20:31.610750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.704666Z",
     "start_time": "2023-07-25T22:20:31.643952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "batch_size = 12\n",
    "block_size = 256  # Number of continues tokens (history) to use for training\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-6\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01\n",
    "warm_up = 10000\n",
    "drop_out = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.722598Z",
     "start_time": "2023-07-25T22:20:31.689354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "dataset = datasets.load_dataset('oscar', 'unshuffled_deduplicated_da')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.789307Z",
     "start_time": "2023-07-25T22:20:31.723733Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean and split text into filechunks of 5000 samples per file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "nlp = spacy.load('da_core_news_sm')\n",
    "def split_sentences(string):\n",
    "    return (' '.join(str(s).strip().split()) for s in nlp(string).sents)\n",
    "\n",
    "def remove_samples_with_invalid_chars(samples, approved_chars):\n",
    "    return (split_sentences(s) for s in samples if all(char in approved_chars for char in s))\n",
    "\n",
    "def remove_sentences_with_invalid_chars(sample, approved_chars):\n",
    "    sentences = split_sentences(sample)\n",
    "    valid_sentences = (sentence for sentence in sentences if len(sentence) > 5 and all(char in approved_chars for char in sentence))\n",
    "    return valid_sentences\n",
    "\n",
    "def chunkify(iterable, size):\n",
    "    iterator = iter(iterable)\n",
    "    for first in iterator:\n",
    "        yield itertools.chain([first], itertools.islice(iterator, size - 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.890174Z",
     "start_time": "2023-07-25T22:20:31.773538Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "approved_chars = {' ', 'e', 'r', 'n', 't', 'i', 'd', 'a', 's', 'l', 'o', 'g', 'k', 'm', 'v', 'f', 'u', 'p', 'b', 'h', '.', ',', 'å', 'æ', 'ø', 'j', 'y', 'D', 'c', '\\n', 'S', '1', '0', '2', '-', 'E', 'A', 'H', 'I', 'F', 'M', 'K', 'B', 'T', 'V', 'N', 'R', 'L', 'P', ':', 'J', 'O', '3', '5', 'G', '4', ')', 'U', '9', '(', 'C', '8', '6', '7', 'x', 'w', '/', '?', '!', '\"', '–', '”', 'z', 'Ø', 'W', 'é', 'Å', 'Y', '|', '…', '&', 'X', \"'\", ';', '’', 'q', '“', 'Æ', '<', '[', '>', ']', '%', 'Z', 'Q', '@', '+', 'ö', '´', '·', '_', 'ä', '*', '‘', 'ü', '§', 'Ã', '一', '½', '—', '$', '=', 'á', '°', '{', '}', 'à', '^', 'è', '~', 'É', 'Ö', 'â', 'ã', 'ß', 'ô', '€', 'ò', 'Ž', '`', 'Ä', 'ù', 'õ', 'Ü', '£', 'ë', '¼'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.891462Z",
     "start_time": "2023-07-25T22:20:31.826730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "s = \"Jeg havde en god tid med Hr. Tim. Han var meget omsorgsful, f.eks. købte han mine nye sko. Han er en rigtig gutterman. Jeg har ikke nogen gode ideer til aftensmad. Quinn har taget en god lur. Emil er god til at bygge med LEGO.\"\n",
    "#ss = split_sentences(s)\n",
    "#fss = str(next(ss))\n",
    "rss = remove_sentences_with_invalid_chars(s, approved_chars)\n",
    "chunkify(rss, 3)\n",
    "list(rss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.989391Z",
     "start_time": "2023-07-25T22:20:31.875511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "clean_samples = remove_samples_with_invalid_chars(dataset['train'][:100_000]['text'], approved_chars)\n",
    "\n",
    "oscar_today_dir = oscar_dir / today\n",
    "oscar_today_dir.mkdir(exist_ok=True)\n",
    "#'</s>'.join(next(chunkfied_samples))\n",
    "for i, text_data in tqdm(enumerate(clean_samples)):\n",
    "    with open(oscar_today_dir / f'clean_samples_{i}.txt', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(text_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:31.990480Z",
     "start_time": "2023-07-25T22:20:31.925036Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "clean_text = (remove_sentences_with_invalid_chars(sample, approved_chars) for sample in tqdm(dataset['train'][:100_000]['text']))\n",
    "samples = chunkify(flatten(clean_text), 5000)\n",
    "\n",
    "for i, text_data in tqdm(enumerate(samples)):\n",
    "    with open(oscar_dir / f'clean_text_{i}.txt', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(text_data).lower())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.088130Z",
     "start_time": "2023-07-25T22:20:31.974644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "oscar_20230630_dir = oscar_dir / \"20230630\"\n",
    "paths = list((oscar_20230630_dir).glob('**/clean_samples_*.txt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.532586Z",
     "start_time": "2023-07-25T22:20:32.030540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiktoken import _educational as ttedu\n",
    "\n",
    "\n",
    "str_pattern = (\n",
    "    r\"\"\" ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "\n",
    "t = ttedu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "d = {'<pad>': 'padding',\n",
    "     '<mask>': 'mask',\n",
    "     '<unk>': 'unknown',\n",
    "     '<b_addr>': 'begin address',\n",
    "     '<i_addr>': 'intermediate address',\n",
    "     '<b_price>': 'begin price',\n",
    "     '<i_price>': 'intermediate price',\n",
    "     '<b_bool>': 'begin of yes/no answer to question',\n",
    "     '<i_bool>': '',\n",
    "     '<b_type>': 'begin property type',\n",
    "     '<i_type': '',\n",
    "     '<b_date>': 'begin date',\n",
    "     '<i_date>': ''}\n",
    "tokenizer.train(files=list(map(str, paths)), vocab_size=vocab_size, min_frequency=2,\n",
    "                special_tokens=list(d.keys()))\n",
    "\n",
    "tokenizer.save_model(str(model_dir / 'byte_level_tokenizer'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.594993Z",
     "start_time": "2023-07-25T22:20:32.533643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "d = {'<pad>': 'padding',\n",
    "     '<mask>': 'mask',\n",
    "     '<unk>': 'unknown',\n",
    "     '<b_addr>': 'begin address',\n",
    "     '<i_addr>': 'intermediate address',\n",
    "     '<b_price>': 'begin price',\n",
    "     '<i_price>': 'intermediate price',\n",
    "     '<b_bool>': 'begin of yes/no answer to question',\n",
    "     '<i_bool>': '',\n",
    "     '<b_type>': 'begin property type',\n",
    "     '<i_type': '',\n",
    "     '<b_date>': 'begin date',\n",
    "     '<i_date>': ''}\n",
    "tokenizer.train(files=list(map(str, paths)), vocab_size=vocab_size, min_frequency=2,\n",
    "                special_tokens=list(d.keys()))\n",
    "\n",
    "bert_tokenizer_path = model_dir / 'bert_tokenizer'\n",
    "bert_tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save_model(str(bert_tokenizer_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.673209Z",
     "start_time": "2023-07-25T22:20:32.579639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "roberta = RobertaTokenizerFast.from_pretrained('model/byte_level_tokenizer')\n",
    "#bert = BertTokenizerFast.from_pretrained('model/bert_tokenizer')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.754019Z",
     "start_time": "2023-07-25T22:20:32.628696Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "with open(oscar_dir / \"clean_text_1.txt\", \"r\") as file_path:\n",
    "    lsamples = file_path.readlines()\n",
    "\n",
    "with open(oscar_dir / \"clean_text_2.txt\", \"r\") as file_path:\n",
    "    lsamples2 = file_path.readlines()\n",
    "\n",
    "with open(oscar_dir / \"clean_samples_0.txt\", \"r\") as file_path:\n",
    "    lsamples3 = file_path.readlines()\n",
    "\n",
    "lsamples = lsamples + lsamples2 + lsamples3\n",
    "\n",
    "first_sample = lsamples[0]\n",
    "first_sentences = [split_sentences(s) for s in lsamples]\n",
    "first_sentences = flatten(first_sentences)\n",
    "counter = Counter(len(t) for t in first_sentences)\n",
    "print(counter)\n",
    "print(\"Minimum length:\", min(counter))\n",
    "print(\"Maximum length:\", max(counter))\n",
    "avg_all = flatten([[k]*v for k,v in counter.items()])\n",
    "print(\"Average length:\", sum(avg_all) / len(avg_all))\n",
    "print(\"Total count:\", sum(counter.values()))\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict(counter), orient='index').reset_index()\n",
    "df = df.rename(columns={\"index\": \"sentence_len\", 0: \"count\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.807455Z",
     "start_time": "2023-07-25T22:20:32.699738Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "test_strs = [\"Jeg havde en god tid med Hr. Tim. Han var meget omsorgsful, f.eks. købte han mine nye sko.\", \"Han er en rigtig gutterman.\", \"Jeg har ikke nogen gode ideer til aftensmad.\",\"Quinn har taget en god lur.\", \"Emil er god til at bygge med LEGO.\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:20:32.808356Z",
     "start_time": "2023-07-25T22:20:32.744574Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "samples = []\n",
    "for file_path in paths:\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        samples.append(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:22:26.909377Z",
     "start_time": "2023-07-25T22:20:32.781035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "training_data, test_data = train_test_split(samples, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:22:26.972630Z",
     "start_time": "2023-07-25T22:22:26.910773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "training_dataset = MaskingDataset(training_data, roberta, block_size)\n",
    "validation_dataset = MaskingDataset(test_data, roberta, block_size)\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_data = iter(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T22:23:10.772959Z",
     "start_time": "2023-07-25T22:23:08.619755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(training_dataset))):\n",
    "    sample, mask, attention_mask = training_dataset[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "\n",
    "ner_samples = [[\"Hej, mit navn er Tim.\", \"Jeg bor i Vejle.\"],[\"Jeg hedder Emil.\", \"Jeg bor i min fars og mors hus.\", \"Det ligger ved Søndermarken.\"]]\n",
    "ner_labels = [[[0, 0, 0, 0, 1],[0, 0, 0, 2]], [[0,0,1],[0,0,0,0,0,0,0,0], [0,0,0,2]]]  # 1 - name, 2 - location\n",
    "ner_training_data, ner_test_data, ner_training_labels, ner_test_labels = train_test_split(ner_samples, ner_labels, test_size=0.20, random_state=42) # Insert NER samples here\n",
    "\n",
    "ner_training_dataset = NerDataset(ner_training_data, ner_training_labels, roberta, block_size)\n",
    "ner_validation_dataset = NerDataset(ner_test_data, ner_test_labels, roberta, block_size)\n",
    "\n",
    "ner_train_dataloader = DataLoader(ner_training_dataset, batch_size=2, shuffle=True)\n",
    "ner_validation_dataloader = DataLoader(ner_validation_dataset, batch_size=2, shuffle=True)\n",
    "ner_sample, ner_label, ner_attention_mask = next(iter(ner_train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:51:13.412051Z",
     "start_time": "2023-07-09T08:51:13.344884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m roberta\u001B[38;5;241m.\u001B[39mdecode(\u001B[43mner_sample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "roberta.decode(ner_sample[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:50:11.898895Z",
     "start_time": "2023-07-09T08:50:11.827488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def subword_level_alignment(offset_mapping):\n",
    "    token_idx = []\n",
    "    count = 0\n",
    "    prev = 0\n",
    "    for om in offset_mapping:\n",
    "        if om == (0,0):\n",
    "            token_idx.append(-1)\n",
    "            continue\n",
    "        start, nprev = om\n",
    "        # There is a space between prev and current token, if there is an offset of 1\n",
    "        count += int(start == prev+1)\n",
    "        token_idx.append(count)\n",
    "        prev = nprev\n",
    "    return token_idx, count+1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T12:20:18.737506Z",
     "start_time": "2023-07-08T12:20:18.641334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n",
    "    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n",
    "\n",
    "    Parameters:\n",
    "    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n",
    "                     - \"tokens\": list of words in a sentence.\n",
    "                     - \"ner_tags\": list of corresponding entity tags for each word.\n",
    "\n",
    "    label_all_tokens (bool): A flag to indicate whether all tokens should have labels.\n",
    "                             If False, only the first token of a word will have a label,\n",
    "                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n",
    "\n",
    "    Returns:\n",
    "    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n",
    "    The function computes precision, recall, F1 score and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the precision, recall, F1 score and accuracy.\n",
    "    \"\"\"\n",
    "    pred_logits, labels = eval_preds\n",
    "\n",
    "    pred_logits = np.argmax(pred_logits, axis=2)\n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "\n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [\n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    results = metrics.performance_measure(true_labels, predictions)\n",
    "\n",
    "    results = metrics.compute(predictions=predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)  # Stores \"what I am/have\"\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)  # Stores \"what am I looking for/interested in\"\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)  # Stores \"If you find me interesting, here is what I will communicate to you\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        B, C = x.shape # batch, n_embed,\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        # Self attention weights\n",
    "        wei = query @ key.transpose(-2,-1) * C ** -0.5\n",
    "        wei = wei.masked_fill_(attention_mask, float('-inf'))\n",
    "        attn = F.softmax(wei, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Perform the weighted aggregation of values\n",
    "        value = self.value(x)\n",
    "        context = attn @ value\n",
    "        return context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MultiHead"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList((Head(head_size, n_embed, dropout) for _ in range(num_heads)))\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        #self.norm = nn.LayerNorm(n_embed, n_embed) <- Norm is just a part of Block instead\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple layer followed by a non-linearity\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Intersperse communication and computation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.sa_heads = MultiHead(num_heads, head_size, n_embed, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.ffwd = FeedForward(n_embed, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.sa_heads(x)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x) # (B, T, C) - each token thinks individually\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, block_size, vocab_size, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*((Block(num_heads, n_embed, dropout)) for _ in range(num_layers)))\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        B, T = x.shape\n",
    "        token_embed = self.token_embedding(x) # (batch_size, time_size, C)\n",
    "        pos_embed = self.position_embedding(torch.arange(T, device=x.device)) # (time_size, C)\n",
    "        x = token_embed + pos_embed # tensors gets batch aligned, so pos_embed: (batch_suze, time_size, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (batch_size, time_size, vocab_size)\n",
    "\n",
    "        if y is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch_size, block_size, vocab_size = logits.shape\n",
    "        # Change layout to use cross_entropy (as it expects (batch, channel))\n",
    "        logits = logits.view(batch_size * block_size, vocab_size) # Flatten two first dims\n",
    "        y = y.view(batch_size*block_size) # Flatten\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    #def __init__(self, num_layers, num_heads, block_size, vocab_size, n_embed, dropout):\n",
    "    def __init__(self, block_size, vocab_size, n_segments, n_embed):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        #self.position_embedding = nn.Embedding(block_size, n_embed)\n",
    "        self.segment_embedding = nn.Embedding(n_segments, n_embed)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
    "\n",
    "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
    "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
    "\n",
    "        token_embed = self.token_embedding(input_tensor)\n",
    "        segment_embed = self.segment_embedding(segment_tensor)\n",
    "        x = token_embed + segment_embed + pos_tensor\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    \"\"\"\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "    \"\"\"\n",
    "    def attention_position(self, dim, input_tensor):\n",
    "        B, T, S = input_tensor.shape\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        pos = torch.arange(sentence_size, dtype=torch.long, device=device)\n",
    "        d = torch.arange(dim, dtype=torch.long, device=device)\n",
    "        d = (2 * d / dim)\n",
    "\n",
    "        pos = pos.unsqueeze(1)\n",
    "        pos = pos / (1e4 ** d)\n",
    "\n",
    "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
    "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
    "\n",
    "        return pos.expand(B, *pos.size())\n",
    "\n",
    "    def numeric_position(self, dim, input_tensor):\n",
    "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
    "        return pos_tensor.expand_as(input_tensor)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = F.scaled_dot_product_attention(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "\n",
    "\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
